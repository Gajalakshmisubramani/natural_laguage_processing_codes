{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAUqe6YQReglmBcqtdprpA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gajalakshmisubramani/natural_laguage_processing_codes/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex.3 sentiment analysis"
      ],
      "metadata": {
        "id": "nDd3SaQgkq7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoEAsjuUkfCq",
        "outputId": "9a366799-23ce-45e5-ef1a-75cc8cfd68f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import classification_report\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "def analyze_sentiment(text):\n",
        "    sentiment_scores = analyzer.polarity_scores(text)\n",
        "    compound_score = sentiment_scores['compound']\n",
        "\n",
        "    if compound_score >= 0.05:\n",
        "        return \"Positive\"\n",
        "    elif compound_score <= -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "test_sentences = [\n",
        "    \"I love this product!\",         # Positive\n",
        "    \"This is the worst experience.\", # Negative\n",
        "    \"I am not sure how I feel.\",     # Neutral\n",
        "    \"Amazing quality and service!\",  # Positive\n",
        "    \"Not happy with the results.\",   # Negative\n",
        "    \"It's okay, nothing special.\"    # Neutral\n",
        "]\n",
        "true_labels = [\"Positive\", \"Negative\", \"Neutral\", \"Positive\", \"Negative\", \"Neutral\"]\n",
        "predicted_labels = [analyze_sentiment(sentence) for sentence in test_sentences]\n",
        "print(test_sentences[0])\n",
        "print(predicted_labels[0])\n",
        "print(classification_report(true_labels, predicted_labels, target_names=[\"Positive\", \"Negative\", \"Neutral\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bZr31fti538",
        "outputId": "2044a208-d0f5-4251-e843-26c749798086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this product!\n",
            "Positive\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.50      1.00      0.67         2\n",
            "    Negative       0.00      0.00      0.00         2\n",
            "     Neutral       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.50      0.67      0.56         6\n",
            "weighted avg       0.50      0.67      0.56         6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex.4 keyword"
      ],
      "metadata": {
        "id": "56yc4yE1ockF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rake-nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkRaCfDhor4j",
        "outputId": "dc744836-cd4e-45cf-f6a6-a04f9081c7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.6)\n",
            "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rake_nltk import Rake\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def extract_keywords_rake(text):\n",
        "    rake = Rake()\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    keywords = rake.get_ranked_phrases()\n",
        "    return keywords\n",
        "def calculate_tfidf_scores(corpus, keywords):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = {}\n",
        "    for keyword in keywords:\n",
        "        for word in keyword.split():\n",
        "            if word in feature_names:\n",
        "                index = feature_names.tolist().index(word)\n",
        "                tfidf_scores[word] = tfidf_matrix[0, index]\n",
        "\n",
        "    return tfidf_scores\n",
        "text = input(\"Enter text for keyword extraction: \")\n",
        "corpus = [text]\n",
        "keywords = extract_keywords_rake(text)\n",
        "print(\"Extracted Keywords (RAKE):\", keywords)\n",
        "tfidf_scores = calculate_tfidf_scores(corpus, keywords)\n",
        "print(\"TF-IDF Scores for Keywords:\", tfidf_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZJr33IioXla",
        "outputId": "8684948b-7c06-4329-cd5c-6ccb03d5afee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text for keyword extraction: hello this is google\n",
            "Extracted Keywords (RAKE): ['hello', 'google']\n",
            "TF-IDF Scores for Keywords: {'hello': 0.5, 'google': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex.5 nlp for other language"
      ],
      "metadata": {
        "id": "DEk7BkdYi7O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "0oeCKChQiZ4C",
        "outputId": "810959c6-a2fb-45f1-c65f-e7087ca2307b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=78878820e90de2817fc17798153236d801ae70c21571b231a119647e3623ba68\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "  Attempting uninstall: googletrans\n",
            "    Found existing installation: googletrans 3.0.0\n",
            "    Uninstalling googletrans-3.0.0:\n",
            "      Successfully uninstalled googletrans-3.0.0\n",
            "Successfully installed googletrans-4.0.0rc1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "googletrans"
                ]
              },
              "id": "7010699fed41408087b1e0a0dd4f1321"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxIcb59CiHJ2",
        "outputId": "2f8691cc-3997-41f8-ee4a-002dadaae3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Language: fr\n",
            "Original Text: Bonjour, comment ça va?\n",
            "Translated Text: Hello, how are you?\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "def translate_text(text, target_language='en'):\n",
        "    detected_language = detect(text)\n",
        "    print(f\"Detected Language: {detected_language}\")\n",
        "    translation = translator.translate(text, dest=target_language)\n",
        "    return translation.text\n",
        "sample_sentence = \"Bonjour, comment ça va?\"\n",
        "target_lang = 'en'\n",
        "translated_text = translate_text(sample_sentence, target_lang)\n",
        "print(\"Original Text:\", sample_sentence)\n",
        "print(\"Translated Text:\", translated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex.7-chatbot"
      ],
      "metadata": {
        "id": "xmY0GEk5riLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def college_chatbot():\n",
        "    print(\"Welcome to the College Query Chatbot!\")\n",
        "    print(\"You can ask about the following topics:\")\n",
        "    print(\"1. Admission Process\")\n",
        "    print(\"2. Courses Offered\")\n",
        "    print(\"3. Campus Facilities\")\n",
        "    print(\"4. Contact Information\")\n",
        "    print(\"5. Exit\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nWhat would you like to know?\").lower()\n",
        "\n",
        "        if \"admission\" in query:\n",
        "            print(\"The admission process involves filling out an online application form and submitting required documents. Check our website for deadlines.\")\n",
        "        elif \"courses\" in query or \"programs\" in query:\n",
        "            print(\"We offer various undergraduate and postgraduate programs including Engineering, Arts, Science, and Business.\")\n",
        "        elif \"facilities\" in query:\n",
        "            print(\"Our campus has a library, computer labs, sports facilities, and student hostels.\")\n",
        "        elif \"contact\" in query:\n",
        "            print(\"You can contact the admissions office at admissions@college.edu or call us at (123) 456-7890.\")\n",
        "        elif \"exit\" in query:\n",
        "            print(\"Thank you for using the College Query Chatbot! Have a great day!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"I'm sorry, I didn't understand your query. Please try asking about admission, courses, facilities, or contact information.\")\n",
        "\n",
        "# Run the chatbot\n",
        "college_chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hooToTQyrDt6",
        "outputId": "383c932e-6378-4da8-f74d-dd9e0759ab44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the College Query Chatbot!\n",
            "You can ask about the following topics:\n",
            "1. Admission Process\n",
            "2. Courses Offered\n",
            "3. Campus Facilities\n",
            "4. Contact Information\n",
            "5. Exit\n",
            "\n",
            "What would you like to know?contact\n",
            "You can contact the admissions office at admissions@college.edu or call us at (123) 456-7890.\n",
            "\n",
            "What would you like to know?exit\n",
            "Thank you for using the College Query Chatbot! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex.8 next word prediction"
      ],
      "metadata": {
        "id": "aD5UsLUHkEgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "corpus = \"I love programming in Python. Python is a great programming language. I love coding in Python.\"\n",
        "words = corpus.lower().split()\n",
        "n = 2\n",
        "bigrams = defaultdict(list)\n",
        "for i in range(len(words) - n):\n",
        "    prefix = ' '.join(words[i:i+n-1])\n",
        "    next_word = words[i+n-1]\n",
        "    bigrams[prefix].append(next_word)\n",
        "def predict_next_word(prefix):\n",
        "    prefix = prefix.lower()\n",
        "    if prefix in bigrams:\n",
        "        return random.choice(bigrams[prefix])\n",
        "    else:\n",
        "        return \"No prediction available.\"\n",
        "input_prefix = \"I\"\n",
        "predicted_word = predict_next_word(input_prefix)\n",
        "print(f\"Input: '{input_prefix}'\")\n",
        "print(f\"Predicted Next Word: '{predicted_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF5T5r3qjEFY",
        "outputId": "303cf2bf-9313-4a27-b37c-47a5a14ea824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'I'\n",
            "Predicted Next Word: 'love'\n"
          ]
        }
      ]
    }
  ]
}